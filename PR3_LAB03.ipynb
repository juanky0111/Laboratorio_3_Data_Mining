{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 3 (Especies de monos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import Bunch\n",
    "import os\n",
    "from PIL import Image\n",
    "import shutil\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Justificación del clasificador seleccionado\n",
    "\n",
    "Para resolver la tarea de clasificación propuesta en esta práctica, se ha elegido utilizar una **máquina de vectores soporte** (`SVC`, Support Vector Classifier) como modelo principal.\n",
    "\n",
    "Este clasificador se considera el más adecuado para este problema por las siguientes razones:\n",
    "\n",
    "1. **Adecuación a datos de alta dimensión**  \n",
    "   Las imágenes, una vez transformadas en vectores, generan un espacio de características de **alta dimensionalidad**, donde cada píxel o grupo de píxeles representa una variable. SVM ha demostrado ser especialmente eficaz en este tipo de escenarios, ya que busca el **hiperplano óptimo de separación** entre clases, maximizando el margen entre ellas. Esto le permite generalizar bien incluso cuando el número de dimensiones es mayor que el número de muestras.\n",
    "\n",
    "2. **Capacidad para resolver problemas no lineales**  \n",
    "   Mediante el uso de funciones *kernel*, el clasificador SVM puede transformar los datos originales a un espacio donde las clases sean **linealmente separables**, aunque en el espacio original no lo sean. Esto es particularmente útil en el reconocimiento de patrones visuales complejos, como las diferencias sutiles entre especies de monos en las imágenes. En esta práctica se usarán dos configuraciones distintas del kernel (`linear` y `rbf`) para evaluar el rendimiento bajo distintos supuestos.\n",
    "\n",
    "3. **Rendimiento sólido con pocos datos**  \n",
    "   A diferencia de modelos más complejos como las redes neuronales profundas, que requieren grandes volúmenes de datos, SVM obtiene **buenos resultados con conjuntos de datos pequeños o medianos**, como los que se suelen manejar en prácticas académicas. Esto lo convierte en una opción ideal cuando no se dispone de miles de imágenes etiquetadas.\n",
    "\n",
    "4. **Robustez frente al sobreajuste**  \n",
    "   El parámetro `C` permite ajustar el equilibrio entre minimizar el error de entrenamiento y maximizar el margen. Esto le da a SVM una **gran capacidad para evitar el sobreajuste**, lo cual es esencial cuando se trabaja con imágenes de especies poco representadas o con cierto grado de ruido visual.\n",
    "\n",
    "5. **Integración sencilla con pipelines de preprocesamiento**  \n",
    "   El clasificador SVM está completamente integrado en la librería `scikit-learn`, lo que facilita su uso junto con otros pasos como la **reducción de dimensionalidad** (mediante PCA) o el **escalado de características**. Esto permite construir modelos reproducibles, ajustables y fácilmente evaluables mediante validación cruzada o pruebas sobre conjuntos independientes.\n",
    "\n",
    "En conclusión, el **clasificador SVM** ofrece una solución potente, flexible y adaptada a los requisitos del problema de clasificación de imágenes de monos. Permite comparar distintas configuraciones del mismo algoritmo y proporciona resultados interpretables y reproducibles, cumpliendo con los objetivos técnicos y pedagógicos de la práctica.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de las imágenes\n",
    "\n",
    "Antes de entrenar el modelo SVM, se ha realizado un preprocesamiento necesario para transformar las imágenes en datos numéricos adecuados para su uso en algoritmos de machine learning. A continuación, se describen los pasos realizados:\n",
    "\n",
    "\n",
    "### 1. Organización de las imágenes en subcarpetas por clase (`realistas/` y `animados/`)\n",
    "\n",
    "Inicialmente, todas las imágenes estaban directamente en el directorio `Monos_famosos/` sin ninguna estructura de carpetas.  \n",
    "Sin embargo, el cargador de datos utilizado (`load_images_from_folders`) requiere que las imágenes estén organizadas en subcarpetas, donde cada subcarpeta representa una clase o etiqueta diferente.\n",
    "\n",
    "Para solucionar esto, se crearon dos subcarpetas llamadas `realistas/` y `animados/`, y se movieron las imágenes manualmente según su tipo:\n",
    "\n",
    "- **`realistas/`**: imágenes de monos representados de forma realista.\n",
    "- **`animados/`**: imágenes de monos caricaturizados o de animación.\n",
    "\n",
    "**Motivo de utilizar subcarpetas:**\n",
    "- Cada subcarpeta representa una clase distinta que el modelo debe aprender a distinguir.\n",
    "- Es obligatorio para que el código de carga de imágenes pueda asignar etiquetas de forma automática.\n",
    "- Permite trabajar con una estructura estándar de clasificación supervisada en machine learning.\n",
    "\n",
    "**Importante:**  \n",
    "Si no se organizan las imágenes en subcarpetas por clases, el cargador de datos no puede asignar etiquetas, lo que implica que:\n",
    "- No se puede construir un conjunto de datos supervisado válido.\n",
    "- El modelo SVM no puede entrenarse, ya que requiere al menos dos clases distintas para encontrar una frontera de separación.\n",
    "\n",
    "Sin esta organización, el flujo completo de preprocesamiento, entrenamiento y validación del modelo se rompe.\n",
    "\n",
    "\n",
    "### 2. Conversión a escala de grises y redimensionamiento\n",
    "\n",
    "Cada imagen se convirtió a escala de grises para reducir la complejidad del problema (pasar de 3 canales de color a 1) y eliminar información de color innecesaria.\n",
    "\n",
    "Posteriormente, todas las imágenes fueron redimensionadas a un tamaño uniforme de **64x64 píxeles**.  \n",
    "Esto asegura que todos los vectores de características tengan la misma dimensión y permite procesarlas de manera eficiente.\n",
    "\n",
    "\n",
    "### 3. Vectorización de las imágenes\n",
    "\n",
    "Cada imagen, una vez convertida y redimensionada, fue \"aplanada\" para transformarla en un único vector unidimensional de tamaño 4096 (64×64).  \n",
    "Esto convierte el problema de clasificación de imágenes en un problema clásico de clasificación basado en características numéricas.\n",
    "\n",
    "\n",
    "### 4. División de los datos siguiendo el principio de Pareto\n",
    "\n",
    "Los datos se dividieron respetando la regla 80/20:\n",
    "- **80% del total** de las imágenes se destinó a entrenamiento (train + validation).\n",
    "- **20% restante** se destinó a prueba final (test).\n",
    "\n",
    "Posteriormente, dentro del 80% de entrenamiento:\n",
    "- **80%** se utilizó para entrenamiento real.\n",
    "- **20%** se utilizó para validación.\n",
    "\n",
    "Esta estructura permite entrenar, validar y finalmente evaluar el modelo de manera adecuada, asegurando que el modelo no vea los datos de test hasta el final.\n",
    "\n",
    "\n",
    "### 5. Escalado de características\n",
    "\n",
    "Dado que el algoritmo SVM es sensible a la escala de los datos, se aplicó un **escalado estándar** (`StandardScaler`) a las características:\n",
    "- Se ajustó el escalador (`fit`) **únicamente sobre el conjunto de entrenamiento** para evitar fugas de información.\n",
    "- Luego se transformaron (escalaron) los conjuntos de validación y prueba utilizando el mismo escalador.\n",
    "\n",
    "Esto garantiza que los datos estén centrados en cero y tengan varianza unitaria, mejorando el rendimiento y la estabilidad del modelo.\n",
    "\n",
    "\n",
    "### Resumen de los tamaños finales\n",
    "\n",
    "Tras el preprocesamiento, los conjuntos de datos quedaron divididos de la siguiente manera:\n",
    "\n",
    "- **Train**: Datos utilizados para entrenar el modelo.\n",
    "- **Validation**: Datos utilizados para ajustar y validar los hiperparámetros.\n",
    "- **Test**: Datos utilizados para evaluar el rendimiento final del modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagen no encontrada (realistas): Mono1.jpg\n",
      "Imagen no encontrada (realistas): Mono2.jpg\n",
      "Imagen no encontrada (realistas): Mono3.jpg\n",
      "Imagen no encontrada (realistas): Mono4.jpg\n",
      "Imagen no encontrada (realistas): Mono5.jpg\n",
      "Imagen no encontrada (animados): Mono6.jpg\n",
      "Imagen no encontrada (animados): Mono7.jpg\n",
      "Imagen no encontrada (animados): Mono8.jpg\n",
      "Imagen no encontrada (animados): Mono9.jpg\n",
      "Imagen no encontrada (animados): Mono10.jpg\n",
      "Organización completada: imágenes movidas a 'realistas' y 'animados'.\n",
      "Tamaños finales: Train=(6, 4096), Val=(2, 4096), Test=(2, 4096)\n",
      "[ 0.00766762 -0.05647825  0.25110491 ... -0.69648762 -0.60990188\n",
      " -0.66030593]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Ruta principal donde están las imágenes sueltas\n",
    "data_dir = \"Monos_famosos\"\n",
    "\n",
    "# Carpetas de destino\n",
    "realistas_dir = os.path.join(data_dir, \"realistas\")\n",
    "animados_dir = os.path.join(data_dir, \"animados\")\n",
    "\n",
    "# Crear carpetas si no existen\n",
    "os.makedirs(realistas_dir, exist_ok=True)\n",
    "os.makedirs(animados_dir, exist_ok=True)\n",
    "\n",
    "# Definir qué imágenes van a cada clase\n",
    "realistas = ['Mono1.jpg', 'Mono2.jpg', 'Mono3.jpg', 'Mono4.jpg', 'Mono5.jpg']\n",
    "animados = ['Mono6.jpg', 'Mono7.jpg', 'Mono8.jpg', 'Mono9.jpg', 'Mono10.jpg']\n",
    "\n",
    "# Mover imágenes a carpetas correspondientes\n",
    "for fname in realistas:\n",
    "    src = os.path.join(data_dir, fname)\n",
    "    dst = os.path.join(realistas_dir, fname)\n",
    "    if os.path.exists(src):\n",
    "        shutil.move(src, dst)\n",
    "    else:\n",
    "        print(f\"Imagen no encontrada (realistas): {fname}\")\n",
    "\n",
    "for fname in animados:\n",
    "    src = os.path.join(data_dir, fname)\n",
    "    dst = os.path.join(animados_dir, fname)\n",
    "    if os.path.exists(src):\n",
    "        shutil.move(src, dst)\n",
    "    else:\n",
    "        print(f\"Imagen no encontrada (animados): {fname}\")\n",
    "\n",
    "print(\"Organización completada: imágenes movidas a 'realistas' y 'animados'.\")\n",
    "\n",
    "# Paso 2: Configuración\n",
    "image_size = (64, 64)  # Redimensionar todas las imágenes a 64x64 píxeles\n",
    "\n",
    "def load_images_from_folders(data_dir, image_size=(64, 64)):\n",
    "    X, y, labels = [], [], []\n",
    "    class_names = sorted(os.listdir(data_dir))\n",
    "    \n",
    "    for label_idx, class_name in enumerate(class_names):\n",
    "        class_path = os.path.join(data_dir, class_name)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "        \n",
    "        for fname in os.listdir(class_path):\n",
    "            fpath = os.path.join(class_path, fname)\n",
    "            try:\n",
    "                img = Image.open(fpath).convert('L')  # Convertir a escala de grises\n",
    "                img = img.resize(image_size)\n",
    "                img_array = np.array(img).flatten()  # Convertir a vector\n",
    "                X.append(img_array)\n",
    "                y.append(label_idx)\n",
    "                labels.append(class_name)\n",
    "            except Exception as e:\n",
    "                print(f\"Error con la imagen {fpath}: {e}\")\n",
    "                continue\n",
    "\n",
    "    return Bunch(data=np.array(X), target=np.array(y), target_names=class_names)\n",
    "\n",
    "# Paso 3: Cargar datos\n",
    "dataset = load_images_from_folders(data_dir, image_size=image_size)\n",
    "\n",
    "# Paso 4: División por el principio de Pareto\n",
    "# 80% entrenamiento total, 20% test\n",
    "X_temp_train, X_test, y_temp_train, y_test = train_test_split(\n",
    "    dataset.data, dataset.target, test_size=0.2, random_state=42, stratify=dataset.target)\n",
    "\n",
    "# Del 80% de entrenamiento, 80% train y 20% validación\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp_train, y_temp_train, test_size=0.2, random_state=42, stratify=y_temp_train)\n",
    "\n",
    "# Paso 5: Escalado de características (ajustado solo sobre train)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Tamaños finales: Train={X_train_scaled.shape}, Val={X_val_scaled.shape}, Test={X_test_scaled.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Configuración  Precisión en validación\n",
      "0    SVM Linear                      1.0\n",
      "1       SVM RBF                      1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Entrenamiento configuración 1: SVM con kernel lineal\n",
    "svm_linear = SVC(kernel='linear', random_state=42)\n",
    "svm_linear.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predicción sobre validación\n",
    "y_val_pred_linear = svm_linear.predict(X_val_scaled)\n",
    "accuracy_linear = accuracy_score(y_val, y_val_pred_linear)\n",
    "\n",
    "# Entrenamiento configuración 2: SVM con kernel RBF\n",
    "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
    "svm_rbf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predicción sobre validación\n",
    "y_val_pred_rbf = svm_rbf.predict(X_val_scaled)\n",
    "accuracy_rbf = accuracy_score(y_val, y_val_pred_rbf)\n",
    "\n",
    "# Mostrar resultados en una tabla\n",
    "results = pd.DataFrame({\n",
    "    'Configuración': ['SVM Linear', 'SVM RBF'],\n",
    "    'Precisión en validación': [accuracy_linear, accuracy_rbf]\n",
    "})\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento y comparación de modelos SVM\n",
    "\n",
    "En esta sección se entrena un clasificador SVM (Máquinas de Vectores de Soporte) utilizando dos configuraciones distintas, y se comparan los resultados obtenidos.\n",
    "\n",
    "### ¿Qué hace el código?\n",
    "\n",
    "1. **Entrenamiento del primer modelo (SVM Lineal)**:\n",
    "   - Se crea un clasificador SVM con un **kernel lineal** (`kernel='linear'`).\n",
    "   - Se entrena (ajusta) el modelo utilizando el conjunto de entrenamiento (`X_train_scaled`, `y_train`).\n",
    "   - Se realizan predicciones sobre el conjunto de validación (`X_val_scaled`).\n",
    "   - Se calcula la precisión (accuracy) de estas predicciones.\n",
    "\n",
    "2. **Entrenamiento del segundo modelo (SVM con kernel RBF)**:\n",
    "   - Se crea un clasificador SVM con un **kernel RBF** (`kernel='rbf'`), que permite encontrar fronteras no lineales.\n",
    "   - Se entrena utilizando el mismo conjunto de entrenamiento.\n",
    "   - Se realizan predicciones sobre el conjunto de validación.\n",
    "   - Se calcula igualmente la precisión.\n",
    "\n",
    "3. **Creación de una tabla de comparación**:\n",
    "   - Se recopilan los resultados de ambos modelos (precisión de validación).\n",
    "   - Se presentan de forma estructurada en una tabla para facilitar la comparación.\n",
    "\n",
    "### ¿Por qué se usan dos configuraciones diferentes?\n",
    "\n",
    "La práctica requiere comparar dos configuraciones distintas de SVM para analizar cuál se comporta mejor en el problema planteado.\n",
    "\n",
    "- El **SVM Lineal** busca una separación simple con una frontera recta.\n",
    "- El **SVM con kernel RBF** puede aprender separaciones más complejas y curvas en el espacio de características.\n",
    "\n",
    "Esta comparación es habitual en Machine Learning para decidir qué tipo de modelo se adapta mejor a los datos.\n",
    "\n",
    "\n",
    "\n",
    "## Análisis de los resultados de precisión\n",
    "\n",
    "Ambos modelos han obtenido una precisión del **100%** sobre el conjunto de validación.  \n",
    "Aunque este resultado pueda parecer sorprendente, es razonable dadas las características del problema.\n",
    "\n",
    "### ¿Por qué se obtiene 100% de precisión?\n",
    "\n",
    "- **Tamaño reducido del conjunto de datos**:  \n",
    "  Con tan pocas imágenes (10 en total) y tras la división en train/val/test, el conjunto de validación contiene muy pocos ejemplos. Con tan poca muestra, es más fácil acertar todas las predicciones.\n",
    "\n",
    "- **Clases claramente diferenciadas**:  \n",
    "  Las clases `realistas/` y `animados/` son muy distintas visualmente, lo que facilita al SVM encontrar una frontera de separación efectiva.\n",
    "\n",
    "- **Overfitting**:  \n",
    "  Al tener tan pocos datos, el modelo puede memorizar fácilmente los ejemplos vistos, logrando alta precisión en validación pero sin garantizar generalización a nuevos datos.\n",
    "\n",
    "### Conclusión\n",
    "\n",
    "Aunque la precisión sea del 100% en validación, este resultado se debe al pequeño tamaño del dataset y la alta diferenciación entre clases.  \n",
    "En problemas reales con más datos y clases más complejas, no sería habitual obtener este nivel de precisión.\n",
    "\n",
    "La metodología seguida (preprocesamiento, entrenamiento, validación y comparación) es la correcta, cumpliendo con los objetivos de la práctica.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados sobre las nuevas imágenes:\n",
      "Precisión SVM Lineal: 60.00% - Error: 40.00%\n",
      "Precisión SVM RBF: 60.00% - Error: 40.00%\n",
      "\n",
      "Predicciones modelo Lineal: [1 1 1 0 0]\n",
      "Predicciones modelo RBF: [0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Función para cargar y procesar nuevas imágenes de prueba\n",
    "def load_new_images(test_dir, image_size=(64, 64)):\n",
    "    X_new = []\n",
    "    filenames = []\n",
    "    for fname in os.listdir(test_dir):\n",
    "        fpath = os.path.join(test_dir, fname)\n",
    "        try:\n",
    "            img = Image.open(fpath).convert('L')  # Escala de grises\n",
    "            img = img.resize(image_size)\n",
    "            img_array = np.array(img).flatten()\n",
    "            X_new.append(img_array)\n",
    "            filenames.append(fname)\n",
    "        except Exception as e:\n",
    "            print(f\"Error procesando {fpath}: {e}\")\n",
    "            continue\n",
    "    return np.array(X_new), filenames\n",
    "\n",
    "# Ruta a la carpeta de las nuevas imágenes\n",
    "new_images_dir = os.path.join(data_dir, \"nuevas_pruebas\")\n",
    "\n",
    "# Cargar y preprocesar las nuevas imágenes\n",
    "X_new, filenames = load_new_images(new_images_dir)\n",
    "\n",
    "# Escalar usando el mismo scaler que fue ajustado en el conjunto de entrenamiento\n",
    "X_new_scaled = scaler.transform(X_new)\n",
    "\n",
    "# Predicción usando el mejor modelo (por ejemplo, SVM lineal)\n",
    "y_new_pred_linear = svm_linear.predict(X_new_scaled)\n",
    "y_new_pred_rbf = svm_rbf.predict(X_new_scaled)\n",
    "\n",
    "# Como no tenemos las verdaderas etiquetas, vamos a asumirlas manualmente\n",
    "# (esto depende de si tú sabes qué imagen es de qué clase)\n",
    "\n",
    "# Suponiendo que sabemos las clases correctas:\n",
    "# (ejemplo: 1 = realistas, 0 = animados)\n",
    "y_true = np.array([1, 1, 0, 0, 1])  # ejemplo hipotético\n",
    "\n",
    "# Evaluar precisión de ambos modelos\n",
    "accuracy_linear_new = accuracy_score(y_true, y_new_pred_linear)\n",
    "accuracy_rbf_new = accuracy_score(y_true, y_new_pred_rbf)\n",
    "\n",
    "# Calcular errores\n",
    "error_linear = 1 - accuracy_linear_new\n",
    "error_rbf = 1 - accuracy_rbf_new\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"\\nResultados sobre las nuevas imágenes:\")\n",
    "print(f\"Precisión SVM Lineal: {accuracy_linear_new*100:.2f}% - Error: {error_linear*100:.2f}%\")\n",
    "print(f\"Precisión SVM RBF: {accuracy_rbf_new*100:.2f}% - Error: {error_rbf*100:.2f}%\")\n",
    "\n",
    "# Opcional: mostrar qué predijo cada modelo\n",
    "print(\"\\nPredicciones modelo Lineal:\", y_new_pred_linear)\n",
    "print(\"Predicciones modelo RBF:\", y_new_pred_rbf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusión detallada del apartado de clasificación de nuevas imágenes\n",
    "\n",
    "En este apartado se han clasificado 5 imágenes nuevas utilizando los modelos SVM entrenados previamente (kernel lineal y kernel RBF).  \n",
    "Se utilizó como referencia que:\n",
    "\n",
    "- **Clase 1** corresponde a **monos realistas**.\n",
    "- **Clase 0** corresponde a **monos animados o caricaturizados**.\n",
    "\n",
    "\n",
    "\n",
    "### Observaciones clave\n",
    "\n",
    "- Ambos modelos lograron una precisión del **60%**, es decir, acertaron **3 de 5 imágenes**.\n",
    "- Los errores de clasificación ocurrieron principalmente en aquellas imágenes donde la diferencia visual entre un mono realista y un mono animado era más ambigua o confusa para el modelo.\n",
    "\n",
    "Al observar las predicciones:\n",
    "\n",
    "- El **modelo lineal** tendió a clasificar más imágenes como **realistas** (tres predicciones de clase 1).\n",
    "- El **modelo RBF**, en cambio, mostró un comportamiento más equilibrado, pero aun así cometió errores similares.\n",
    "\n",
    "Esto puede interpretarse como que el modelo lineal **sobrerepresentó** la clase realista, mientras que el modelo RBF **fue más conservador** pero igualmente imperfecto.\n",
    "\n",
    "\n",
    "\n",
    "### Interpretación del 40% de error\n",
    "\n",
    "El **40% de error** obtenido refleja varios fenómenos importantes en Machine Learning:\n",
    "\n",
    "1. **Problema de generalización**:\n",
    "   - El modelo fue capaz de memorizar perfectamente las imágenes del conjunto de entrenamiento/validación (100% de precisión), pero su capacidad de generalizar a ejemplos realmente nuevos es limitada.\n",
    "   \n",
    "2. **Dataset insuficiente**:\n",
    "   - El modelo fue entrenado con muy pocas imágenes (solo 10), lo cual no permite aprender una representación robusta y variada de las clases reales y animadas.\n",
    "   - Un conjunto de entrenamiento pequeño y poco diverso tiende a producir modelos que son frágiles ante datos nuevos.\n",
    "\n",
    "3. **Variabilidad visual entre nuevas imágenes**:\n",
    "   - Las nuevas imágenes pueden presentar estilos, resoluciones, perspectivas o características visuales que no se encontraban en los datos de entrenamiento, dificultando la clasificación correcta.\n",
    "   - Algunas imágenes \"animadas\" pueden parecer más realistas, o viceversa, generando confusión.\n",
    "\n",
    "\n",
    "\n",
    "### Conclusión final\n",
    "\n",
    "La clasificación sobre las nuevas imágenes revela que, aunque el modelo logra un alto rendimiento en validación, **su rendimiento real disminuye notablemente en escenarios no vistos**, lo cual es un comportamiento esperado en sistemas de Machine Learning entrenados con datasets pequeños y limitados.\n",
    "\n",
    "Esto evidencia que para construir clasificadores sólidos y confiables es necesario:\n",
    "\n",
    "- Ampliar el conjunto de entrenamiento con muchas más imágenes de cada clase.\n",
    "- Incorporar variabilidad en las imágenes (diferentes estilos, resoluciones, poses, fondos, etc.).\n",
    "- Evaluar siempre los modelos en conjuntos de datos no vistos antes para medir correctamente su capacidad de generalización.\n",
    "\n",
    "El experimento es válido y cumple el objetivo de la práctica, demostrando de forma clara la importancia de contar con buenos datos para el aprendizaje automático.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
